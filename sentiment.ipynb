{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from scikeras) (3.7.0)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from scikeras) (1.6.0)\n",
      "Requirement already satisfied: packaging in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (24.2)\n",
      "Requirement already satisfied: rich in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
      "Requirement already satisfied: ml-dtypes in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
      "Requirement already satisfied: h5py in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
      "Requirement already satisfied: numpy in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
      "Requirement already satisfied: optree in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.13.1)\n",
      "Requirement already satisfied: namex in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: absl-py in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\vs code\\data science\\dsenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout,Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"News_Sentiment_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(848, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_title</th>\n",
       "      <th>reddit_title</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mark Cuban launches generic drug company</td>\n",
       "      <td>Billionaire Mark Cuban just launched a drug co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Billionaire investor and Shark Tank star Mark ...</td>\n",
       "      <td>https://www.beckershospitalreview.com/pharmacy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From Defendant to Defender: One Wrongfully Con...</td>\n",
       "      <td>Man falsely imprisoned for 10 years, uses pris...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Attorney Jarrett Adams recently helped overtur...</td>\n",
       "      <td>https://www.nbcnews.com/news/us-news/defendant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon Tribe Wins Lawsuit Against Big Oil, Sav...</td>\n",
       "      <td>Amazon tribe wins legal battle against oil com...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The Amazon Rainforest is well known across the...</td>\n",
       "      <td>https://www.disclose.tv/amazon-tribe-wins-laws...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newark police: No officer fired a single shot ...</td>\n",
       "      <td>Newark police: No officer fired a single shot ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Newark police: No officer fired a single shot ...</td>\n",
       "      <td>https://newjersey.news12.com/newark-police-no-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ingen barn døde i trafikken i 2019</td>\n",
       "      <td>No children died in traffic accidents in Norwa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I 1970 døde det 560 mennesker i den norske tra...</td>\n",
       "      <td>https://www.nrk.no/trondelag/ingen-barn-dode-i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          news_title  \\\n",
       "0           Mark Cuban launches generic drug company   \n",
       "1  From Defendant to Defender: One Wrongfully Con...   \n",
       "2  Amazon Tribe Wins Lawsuit Against Big Oil, Sav...   \n",
       "3  Newark police: No officer fired a single shot ...   \n",
       "4                 Ingen barn døde i trafikken i 2019   \n",
       "\n",
       "                                        reddit_title  sentiment  \\\n",
       "0  Billionaire Mark Cuban just launched a drug co...        1.0   \n",
       "1  Man falsely imprisoned for 10 years, uses pris...        1.0   \n",
       "2  Amazon tribe wins legal battle against oil com...        1.0   \n",
       "3  Newark police: No officer fired a single shot ...        1.0   \n",
       "4  No children died in traffic accidents in Norwa...        1.0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Billionaire investor and Shark Tank star Mark ...   \n",
       "1  Attorney Jarrett Adams recently helped overtur...   \n",
       "2  The Amazon Rainforest is well known across the...   \n",
       "3  Newark police: No officer fired a single shot ...   \n",
       "4  I 1970 døde det 560 mennesker i den norske tra...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.beckershospitalreview.com/pharmacy...  \n",
       "1  https://www.nbcnews.com/news/us-news/defendant...  \n",
       "2  https://www.disclose.tv/amazon-tribe-wins-laws...  \n",
       "3  https://newjersey.news12.com/newark-police-no-...  \n",
       "4  https://www.nrk.no/trondelag/ingen-barn-dode-i...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news_title      0\n",
       "reddit_title    0\n",
       "sentiment       0\n",
       "text            0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1.0    748\n",
       "0.0    100\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "texts = df['text'].values  # News articles\n",
    "labels = df['sentiment'].values  # Sentiment labels (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "max_vocab_size = 10000  # The number of unique words to keep in the tokenizer\n",
    "max_sequence_length = 300  # Maximum sequence length for padding\n",
    "embedding_dim = 100  # Word embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure consistent input size\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VS CODE\\Data Science\\dsenv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(units=128, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 544ms/step - accuracy: 0.8244 - loss: 0.6525 - val_accuracy: 0.8471 - val_loss: 0.5073\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 488ms/step - accuracy: 0.8937 - loss: 0.3419 - val_accuracy: 0.8471 - val_loss: 0.4358\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 506ms/step - accuracy: 0.8886 - loss: 0.3261 - val_accuracy: 0.8471 - val_loss: 0.4405\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 505ms/step - accuracy: 0.9014 - loss: 0.2624 - val_accuracy: 0.8471 - val_loss: 0.4923\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 471ms/step - accuracy: 0.9081 - loss: 0.1890 - val_accuracy: 0.8471 - val_loss: 0.5943\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.8638 - loss: 0.4020\n",
      "Test accuracy: 84.71%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VS CODE\\Data Science\\dsenv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the Bidirectional LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(Bidirectional(LSTM(units=128)))  # Bidirectional LSTM\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 877ms/step - accuracy: 0.7685 - loss: 0.6233 - val_accuracy: 0.8471 - val_loss: 0.6701\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 793ms/step - accuracy: 0.8876 - loss: 0.4089 - val_accuracy: 0.8471 - val_loss: 0.4311\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 757ms/step - accuracy: 0.8954 - loss: 0.3277 - val_accuracy: 0.8471 - val_loss: 0.4484\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 785ms/step - accuracy: 0.8801 - loss: 0.3286 - val_accuracy: 0.8471 - val_loss: 0.4301\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 786ms/step - accuracy: 0.8819 - loss: 0.2822 - val_accuracy: 0.8471 - val_loss: 0.4602\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 801ms/step - accuracy: 0.9021 - loss: 0.1879 - val_accuracy: 0.8412 - val_loss: 0.4786\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 832ms/step - accuracy: 0.9814 - loss: 0.0784 - val_accuracy: 0.8353 - val_loss: 0.5567\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.8638 - loss: 0.3959\n",
      "Test accuracy: 84.71%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add EarlyStopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VS CODE\\Data Science\\dsenv\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.6599 - loss: 0.6907 - val_accuracy: 0.8471 - val_loss: 0.6864\n",
      "Epoch 2/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 186ms/step - accuracy: 0.8905 - loss: 0.6828 - val_accuracy: 0.8471 - val_loss: 0.6792\n",
      "Epoch 3/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 202ms/step - accuracy: 0.9024 - loss: 0.6735 - val_accuracy: 0.8471 - val_loss: 0.6695\n",
      "Epoch 4/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 186ms/step - accuracy: 0.8845 - loss: 0.6619 - val_accuracy: 0.8471 - val_loss: 0.6548\n",
      "Epoch 5/5\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 196ms/step - accuracy: 0.8959 - loss: 0.6414 - val_accuracy: 0.8471 - val_loss: 0.6301\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8638 - loss: 0.6267\n",
      "Test accuracy: 84.71%\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 114ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy score: 84.71%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"News_Sentiment_dataset.csv\")\n",
    "\n",
    "# Data Preprocessing\n",
    "texts = df['text'].values  # News articles\n",
    "labels = df['sentiment'].values  # Sentiment labels (0 or 1)\n",
    "\n",
    "# Tokenize the text data using a simple tokenizer (this will not use BERT's tokenizer)\n",
    "tokenizer = Tokenizer(num_words=10000)  # Limit vocab size to 10,000 words\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "max_sequence_length = 256  # You can adjust this value as needed\n",
    "X = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels into TensorFlow tensors\n",
    "train_labels = tf.convert_to_tensor(y_train)\n",
    "test_labels = tf.convert_to_tensor(y_test)\n",
    "\n",
    "# Define the deep LSTM model\n",
    "def create_deep_lstm_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=10000, output_dim=128, input_length=max_sequence_length))\n",
    "    \n",
    "    # First LSTM layer with dropout\n",
    "    model.add(LSTM(128, return_sequences=True))  # Return sequences for the next LSTM layer\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(64))  # No return_sequences, because it's the last LSTM layer\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Dense layer for binary classification (sigmoid activation for binary labels)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_deep_lstm_model()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, train_labels, epochs=5, batch_size=32, validation_data=(X_test, test_labels))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, test_labels)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions (optional)\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = (predictions > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "# Compute accuracy score\n",
    "accuracy = accuracy_score(y_test, predicted_labels)\n",
    "print(f\"Final Accuracy score: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('deep_lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856ms/step\n",
      "The sentiment of the article is: Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved deep LSTM model\n",
    "model = load_model('deep_lstm_model.h5')\n",
    "\n",
    "# Tokenizer used during training (you need to fit it on the training data during the training phase)\n",
    "tokenizer = Tokenizer(num_words=10000)  # Same num_words parameter used during training\n",
    "\n",
    "# Function to predict sentiment of a new news article\n",
    "def predict_sentiment(news_text: str):\n",
    "    # Tokenize the new input text\n",
    "    sequences = tokenizer.texts_to_sequences([news_text])\n",
    "\n",
    "    # Pad the sequences to the same length used during training\n",
    "    max_sequence_length = 256  # Adjust this if necessary (should be the same as during training)\n",
    "    padded_sequence = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(padded_sequence)\n",
    "\n",
    "    # Return the result (0 or 1)\n",
    "    if prediction > 1:\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "\n",
    "# Example usage of the prediction function\n",
    "new_article = \"One killed after Tesla Cybertruck catches fire and explodes outside Trump’s Las Vegas hotel - PBS NewsHour\"\n",
    "\n",
    "sentiment = predict_sentiment(new_article)\n",
    "print(f\"The sentiment of the article is: {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
